{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeoratileShongoane/AML/blob/main/Spark_(1)_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1fx3FedRrga"
      },
      "source": [
        "***\n",
        "# **Data Science in Practise**: Assignment 2\n",
        "#### __Name:__ Keoratile Shongoane\n",
        "#### __Student number:__ 1389986\n",
        "\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assWwxeEY6J4"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n",
        "# Table of Contents\n",
        "\n",
        "- [Imports](#imports)\n",
        "- [1. Starting Spark Session](#1-starting-spark-session)\n",
        "- [2. Preprocessing](#2-preprocessing)\n",
        "  - [2.1 Importing Data](#21-importing-data)\n",
        "  - [2.2 Viewing data types](#22-viewing-data-types)\n",
        "  - [2.3 Dropping missing values](#23-dropping-missing-values)\n",
        "  - [2.4 Transposing Data](#24-transposing-data)\n",
        "  - [2.5 Calculating Descriptive statistics](#25-calculating-descriptive-statistics)\n",
        "  - [2.6 Encoding](#26-encoding)\n",
        "  - [2.7 Transforming Features](#27-transforming-features)\n",
        "  - [2.8 Encoding Target Column](#28-encoding-target-column)\n",
        "  - [2.9 Train-Test Split](#29-train-test-split)\n",
        "- [3. Model Training](#3-model-training)\n",
        "  - [3.1 Random Forest Classifier](#31-random-forest-classifier)\n",
        "  - [3.2 Decision Tree Classifier](#32-decision-tree-classifier)\n",
        "- [4. Model Evaluation](#4-model-evaluation)\n",
        "  - [4.1 Random Forest Classifier](#41-random-forest-classifier)\n",
        "  - [4.2 Decision Tree Classifier](#42-decision-tree-classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC5vMHMOYTli"
      },
      "source": [
        "# __Imports__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKidvvFCH-SA",
        "outputId": "44a8df61-acf4-4520-f3d9-180f7ad9da15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=fe6453232e7c469302aad7951c4b0551538524bfb8da305fadc208e15662208d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrlhuA42R8Jx"
      },
      "source": [
        "The cell below imports spark, starts a spark session, and then prints out the details of the running spark instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2BCHGQ4IWP-"
      },
      "outputs": [],
      "source": [
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession, column\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.types import FloatType\n",
        "import pyspark.sql.functions as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH8HmzElYTlk"
      },
      "source": [
        "# __1. Starting Spark Session__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogZkw43Rs-Me"
      },
      "outputs": [],
      "source": [
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.appName('ml-income').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2wYqn58YTlk"
      },
      "source": [
        "# __2. Proprocessing__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cttPFmJ3YTlk"
      },
      "source": [
        "### __2.1 Importing Data__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUhx6fnsHmR8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "20ccc422-b4fd-4d1c-c69d-c7a2e8d6aba3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/income.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2772587e5300>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'income.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# df = spark.read.csv('C:/User/tshol/Downloads/income.csv', header=True, nullValue='?',ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True, inferSchema=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/income.csv."
          ]
        }
      ],
      "source": [
        "df = spark.read.csv('income.csv', header=True, nullValue='?',ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True, inferSchema=True)\n",
        "# df = spark.read.csv('C:/User/tshol/Downloads/income.csv', header=True, nullValue='?',ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True, inferSchema=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPheINtYTll"
      },
      "source": [
        "### __2.2 Viewing data types__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKpeyjEhCplc"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAKr32VYTll"
      },
      "source": [
        "### __2.3 Dropping missing values__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQEh25j8RLGd"
      },
      "outputs": [],
      "source": [
        "df = df.na.drop()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTSlSMdpYTll"
      },
      "source": [
        "### __2.4 Transposing Data__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN9NJtUSTShn"
      },
      "outputs": [],
      "source": [
        "\n",
        "pd.DataFrame(df.take(5),columns=df.columns).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGOR99jSd33A"
      },
      "source": [
        "### __2.5 Calculating Descriptive statistics__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v3cAHd2T24o"
      },
      "outputs": [],
      "source": [
        "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n",
        "df.select(numeric_features).describe().toPandas().transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppeFP1Awjdor"
      },
      "source": [
        " ### __2.6 Encoding__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0t16r4AZooa"
      },
      "outputs": [],
      "source": [
        "label_stringIdx= StringIndexer(inputCols=[\"workclass\", \"education\", \"marital_status\",\"occupation\", \"relationship\", \"race\",\"sex\",\"citizenship\"], outputCols=[\"label_workclass\", \"label_education\", \"label_marital_status\", \"label_occupation\", \"label_relationship\",\"label_race\", \"label_sex\", \"label_citizenship\"])\n",
        "df = label_stringIdx.fit(df).transform(df)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbhzFfOdj_fO"
      },
      "source": [
        "### __2.7 Transforming Features__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KFl70kJWa4B"
      },
      "outputs": [],
      "source": [
        "#using the vector assember to feature transform\n",
        "\n",
        "numericCols =[\"label_workclass\", \"label_education\", \"label_marital_status\", \"label_occupation\", \"label_relationship\",\"label_race\", \"label_sex\", \"label_citizenship\"]\n",
        "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS-eOuA_k1P9"
      },
      "source": [
        "### __2.8 Encoding Target Column__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsums0t_buev"
      },
      "outputs": [],
      "source": [
        "labels = StringIndexer(inputCol=\"income_class\", outputCol=\"label\")\n",
        "df = labels.fit(df).transform(df)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLQ09gepkUsw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbeWjeTxM9tg"
      },
      "outputs": [],
      "source": [
        "# pd.DataFrame(df.take(100),columns=df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTz6Zk4QlZYw"
      },
      "source": [
        "### __2.9 Train-Test Split__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-hY6G1JcHhq"
      },
      "outputs": [],
      "source": [
        "#splitting our datasets\n",
        "train, test = df.randomSplit([0.7,0.3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv2PtloEN0xF"
      },
      "outputs": [],
      "source": [
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBmHggT2lkNA"
      },
      "source": [
        "# __3. Model Training__\n",
        "\n",
        "#### Here, I train 2 models, namely: `Decision Tree Classifier` and `Random Forest Classifier`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zUaerBOmJ6n"
      },
      "source": [
        "### __3.1 Random Forest Classifier__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_b1Yui_cyXy"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", maxBins=41)\n",
        "modelrf = rf.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A54N51D6mf1w"
      },
      "outputs": [],
      "source": [
        "#The RandomnForest Predictor\n",
        "predictionsrf = modelrf.transform(test)\n",
        "predictionsrf.select(\"features\", \"label\").show(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iUUdNIgmTI-"
      },
      "source": [
        "### __3.2 Decision Tree Classifier__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STrX2H5Rys25"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxBins=41)\n",
        "modeldt = dt.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn6hFM2i4eTx"
      },
      "outputs": [],
      "source": [
        "#the DecisionTree Prediction\n",
        "predictionsdt = modeldt.transform(test)\n",
        "predictionsdt.select(\"features\", \"label\").show(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSXFKNxKm7mh"
      },
      "source": [
        "# __4. Model Evaluation__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-ripOaom_4L"
      },
      "source": [
        "### __4.1 Random Forest Classifier__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6FV8NkQifrn"
      },
      "outputs": [],
      "source": [
        "# Initialize evaluators for RandomForest\n",
        "evaluator_accuracy_rf = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "evaluator_precision_rf = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "evaluator_recall_rf = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "evaluator_f1_rf = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "# Evaluate metrics for RandomForest\n",
        "accuracy_rf = evaluator_accuracy_rf.evaluate(predictionsrf)\n",
        "precision_rf = evaluator_precision_rf.evaluate(predictionsrf)\n",
        "recall_rf = evaluator_recall_rf.evaluate(predictionsrf)\n",
        "f1_rf = evaluator_f1_rf.evaluate(predictionsrf)\n",
        "\n",
        "print(\"RandomForest Metrics:\\n\")\n",
        "print(f\"Accuracy = {accuracy_rf}\")\n",
        "print(f\"Precision = {precision_rf}\")\n",
        "print(f\"Recall = {recall_rf}\")\n",
        "print(f\"F1 Score = {f1_rf}\")\n",
        "print(f\"Test Error = {1.0 - accuracy_rf}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7CqYT6GTF42"
      },
      "outputs": [],
      "source": [
        "#confusionmetric\n",
        "preds = predictionsrf.select(['prediction','label']).withColumn('label', F.col('label').cast(FloatType()))\n",
        "\n",
        "preds = preds.select(['prediction', 'label'])\n",
        "metrics = MulticlassMetrics(preds.rdd.map(tuple))\n",
        "print(metrics.confusionMatrix().toArray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ8oMmFRnJdM"
      },
      "source": [
        "### __4.2 Decision Tree Classifier__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuyvlI9tnOLs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize evaluators for DecisionTree\n",
        "evaluator_accuracy_dt = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "evaluator_precision_dt = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "evaluator_recall_dt = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "evaluator_f1_dt = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "\n",
        "# Evaluate metrics for DecisionTree\n",
        "accuracy_dt = evaluator_accuracy_dt.evaluate(predictionsdt)\n",
        "precision_dt = evaluator_precision_dt.evaluate(predictionsdt)\n",
        "recall_dt = evaluator_recall_dt.evaluate(predictionsdt)\n",
        "f1_dt = evaluator_f1_dt.evaluate(predictionsdt)\n",
        "\n",
        "print(\"DecisionTree Metrics:\\n\")\n",
        "print(f\"Accuracy = {accuracy_dt}\")\n",
        "print(f\"Precision = {precision_dt}\")\n",
        "print(f\"Recall = {recall_dt}\")\n",
        "print(f\"F1 Score = {f1_dt}\")\n",
        "print(f\"Test Error = {1.0 - accuracy_dt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsdeC9SQoQh8"
      },
      "outputs": [],
      "source": [
        "preds = predictionsdt.select(['prediction','label']).withColumn('label', F.col('label').cast(FloatType()))\n",
        "\n",
        "preds = preds.select(['prediction', 'label'])\n",
        "metrics = MulticlassMetrics(preds.rdd.map(tuple))\n",
        "print(metrics.confusionMatrix().toArray())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}